import 'dart:async';
import 'dart:io';
import 'dart:convert';
import 'package:flutter/foundation.dart';
import 'package:record/record.dart';
import 'package:path_provider/path_provider.dart';
import 'package:permission_handler/permission_handler.dart';
import 'package:http/http.dart' as http;

/// Service de reconnaissance vocale avec int√©gration Groq Whisper
/// G√®re l'enregistrement, la transcription et la visualisation audio
class VoiceRecognitionService {
  final AudioRecorder _recorder = AudioRecorder();

  // √âtat de l'enregistrement
  bool _isRecording = false;
  String? _currentAudioPath;

  // Stream pour les amplitudes audio (pour la visualisation)
  final StreamController<double> _amplitudeController = StreamController<double>.broadcast();
  Stream<double> get amplitudeStream => _amplitudeController.stream;

  Timer? _amplitudeTimer;

  // Configuration Groq API (GRATUIT)
  static const String _groqApiKey = '';
  static const String _whisperEndpoint = 'https://api.groq.com/openai/v1/audio/transcriptions';

  VoiceRecognitionService();

  /// V√©rifie et demande les permissions microphone
  Future<bool> _checkPermissions() async {
    final status = await Permission.microphone.status;

    if (status.isDenied || status.isPermanentlyDenied) {
      final result = await Permission.microphone.request();
      return result.isGranted;
    }

    return status.isGranted;
  }

  /// D√©marre l'enregistrement audio
  Future<bool> startRecording() async {
    try {
      // V√©rifier les permissions
      final hasPermission = await _checkPermissions();
      if (!hasPermission) {
        debugPrint('‚ùå Permission microphone refus√©e');
        return false;
      }

      // G√©n√©rer un nom de fichier unique
      final directory = await getApplicationDocumentsDirectory();
      final timestamp = DateTime.now().millisecondsSinceEpoch;
      _currentAudioPath = '${directory.path}/recording_$timestamp.m4a';

      // Configuration de l'enregistrement
      const config = RecordConfig(
        encoder: AudioEncoder.aacLc,
        bitRate: 128000,
        sampleRate: 44100,
        numChannels: 1,
      );

      // D√©marrer l'enregistrement
      await _recorder.start(config, path: _currentAudioPath!);
      _isRecording = true;

      // D√©marrer la surveillance de l'amplitude
      _startAmplitudeMonitoring();

      debugPrint('‚úÖ Enregistrement d√©marr√©: $_currentAudioPath');
      return true;

    } catch (e) {
      debugPrint('‚ùå Erreur startRecording: $e');
      _isRecording = false;
      return false;
    }
  }

  /// Surveille l'amplitude audio pour la visualisation
  void _startAmplitudeMonitoring() {
    _amplitudeTimer?.cancel();

    _amplitudeTimer = Timer.periodic(const Duration(milliseconds: 100), (timer) async {
      if (!_isRecording) {
        timer.cancel();
        return;
      }

      try {
        final amplitude = await _recorder.getAmplitude();
        final normalized = (amplitude.current + 160) / 160;
        final clamped = normalized.clamp(0.0, 1.0);
        _amplitudeController.add(clamped);
      } catch (e) {
        debugPrint('‚ö†Ô∏è Erreur amplitude: $e');
      }
    });
  }

  /// Met en pause l'enregistrement
  Future<void> pauseRecording() async {
    if (!_isRecording) return;

    try {
      await _recorder.pause();
      _isRecording = false;
      _amplitudeTimer?.cancel();
      _amplitudeController.add(0.0);

      debugPrint('‚è∏Ô∏è Enregistrement en pause');
    } catch (e) {
      debugPrint('‚ùå Erreur pauseRecording: $e');
    }
  }

  /// Reprend l'enregistrement
  Future<void> resumeRecording() async {
    if (_isRecording) return;

    try {
      await _recorder.resume();
      _isRecording = true;
      _startAmplitudeMonitoring();

      debugPrint('‚ñ∂Ô∏è Enregistrement repris');
    } catch (e) {
      debugPrint('‚ùå Erreur resumeRecording: $e');
    }
  }

  /// Arr√™te l'enregistrement et retourne le chemin du fichier
  Future<String?> stopRecording() async {
    try {
      _amplitudeTimer?.cancel();
      _amplitudeController.add(0.0);

      final path = await _recorder.stop();
      _isRecording = false;

      debugPrint('‚èπÔ∏è Enregistrement arr√™t√©: $path');
      return path;

    } catch (e) {
      debugPrint('‚ùå Erreur stopRecording: $e');
      _isRecording = false;
      return null;
    }
  }

  /// Transcrit l'audio avec Groq Whisper API (GRATUIT)
  Future<String?> transcribeAudio(String audioPath) async {
    try {
      debugPrint('üéØ D√©but transcription avec Groq Whisper...');

      final audioFile = File(audioPath);

      if (!await audioFile.exists()) {
        debugPrint('‚ùå Fichier audio introuvable: $audioPath');
        return null;
      }

      // V√©rifier la taille du fichier
      final fileSize = await audioFile.length();
      debugPrint('üì¶ Taille fichier: ${(fileSize / 1024 / 1024).toStringAsFixed(2)} MB');

      if (fileSize > 25 * 1024 * 1024) {
        debugPrint('‚ùå Fichier trop volumineux (max 25 MB)');
        return null;
      }

      // Pr√©parer la requ√™te multipart
      final request = http.MultipartRequest('POST', Uri.parse(_whisperEndpoint));

      // Headers
      request.headers['Authorization'] = 'Bearer $_groqApiKey';

      // Fichier audio
      request.files.add(
        await http.MultipartFile.fromPath(
          'file',
          audioPath,
          filename: 'audio.m4a',
        ),
      );

      // Param√®tres
      request.fields['model'] = 'whisper-large-v3';
      request.fields['language'] = 'fr';
      request.fields['response_format'] = 'json';
      request.fields['temperature'] = '0.0';

      // Envoyer la requ√™te
      debugPrint('üì§ Envoi √† Groq Whisper API...');
      final streamedResponse = await request.send();
      final response = await http.Response.fromStream(streamedResponse);

      debugPrint('üì• R√©ponse API: ${response.statusCode}');

      if (response.statusCode == 200) {
        final jsonResponse = json.decode(response.body);
        final transcription = jsonResponse['text'] as String?;

        if (transcription != null && transcription.isNotEmpty) {
          // ‚úÖ CORRECTION : Afficher seulement les 50 premiers caract√®res SI le texte est assez long
          final preview = transcription.length > 50
              ? '${transcription.substring(0, 50)}...'
              : transcription;
          debugPrint('‚úÖ Transcription r√©ussie: $preview');
          return transcription;
        } else {
          debugPrint('‚ö†Ô∏è Transcription vide');
          return null;
        }

      } else {
        debugPrint('‚ùå Erreur API Whisper: ${response.statusCode}');
        debugPrint('üìÑ Response body: ${response.body}');

        // Analyser l'erreur pour donner un message utile
        try {
          final errorJson = json.decode(response.body);
          debugPrint('üí¨ Message d\'erreur: ${errorJson['error']}');
        } catch (_) {
          // Pas de JSON dans l'erreur
        }

        return null;
      }

    } catch (e, stackTrace) {
      debugPrint('‚ùå Erreur transcription: $e');
      debugPrint('üìç StackTrace: $stackTrace');
      return null;
    }
  }

  /// Supprime le fichier audio enregistr√©
  Future<void> deleteRecording() async {
    if (_currentAudioPath == null) return;

    try {
      final file = File(_currentAudioPath!);
      if (await file.exists()) {
        await file.delete();
        debugPrint('üóëÔ∏è Fichier audio supprim√©');
      }
      _currentAudioPath = null;
    } catch (e) {
      debugPrint('‚ùå Erreur deleteRecording: $e');
    }
  }

  /// Nettoyage
  void dispose() {
    _amplitudeTimer?.cancel();
    _amplitudeController.close();
    _recorder.dispose();
  }
}